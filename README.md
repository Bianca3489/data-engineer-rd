# Desafio TÃ©cnico de Engenharia de Dados - Email Marketing Funnel

## ğŸ“‹ Resumo do Projeto
Este projeto consiste na construÃ§Ã£o de um pipeline de dados (ELT) para processar mÃ©tricas de Email Marketing. O objetivo Ã© ingerir dados brutos, tratÃ¡-los, validar sua qualidade e entregar tabelas analÃ­ticas para acompanhamento de metas (Leads, MQLs, SALs) e conversÃµes.

A soluÃ§Ã£o foi implementada utilizando **Google BigQuery** como Data Warehouse, **Python** para ingestÃ£o e **SQL** para transformaÃ§Ãµes seguindo a arquitetura Medallion (Bronze, Silver, Gold).

## ğŸ› ï¸ Tecnologias Utilizadas
* **Linguagem:** Python 3.9+ & SQL (Standard dialect)
* **Data Warehouse:** Google BigQuery
* **OrquestraÃ§Ã£o:** Apache Airflow (DAGs incluÃ­das)
* **Versionamento:** Git

## ğŸ—ï¸ Arquitetura de Dados (Medallion)

O pipeline foi dividido em trÃªs camadas lÃ³gicas:

1.  **Bronze (Raw):** IngestÃ£o dos arquivos CSV (`raw_funnel`, `raw_metas`) sem tratamento.
2.  **Silver (Clean):**
    * DeduplicaÃ§Ã£o de registros (mantendo o mais recente).
    * ConversÃ£o de tipos (Casting).
    * PadronizaÃ§Ã£o de Timezone (UTC -> America/Sao_Paulo).
    * **Data Quality:** RemoÃ§Ã£o de inconsistÃªncias temporais (ex: MQL criado antes do Lead).
3.  **Gold (Delivery):**
    * CÃ¡lculo de mÃ©tricas acumuladas (Month-to-Date).
    * Pivotagem da tabela de metas para comparaÃ§Ã£o diÃ¡ria.
    * Join entre dados realizados (Actual) e orÃ§ados (Budget).

## ğŸš€ Como Executar o Projeto

### PrÃ©-requisitos
* Conta no Google Cloud Platform (GCP) com BigQuery habilitado.
* Service Account com permissÃ£o de `BigQuery Admin`.
* Python 3 instalado.

### Passo a Passo

1.  **InstalaÃ§Ã£o das dependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **ConfiguraÃ§Ã£o de Credenciais:**
    * Coloque o arquivo `credentials.json` (Service Account Key) na raiz do projeto.
    * *Nota: Por questÃµes de seguranÃ§a, este arquivo nÃ£o estÃ¡ versionado no Git.*

3.  **ExecuÃ§Ã£o Manual (IngestÃ£o):**
    ```bash
    python src/ingestion.py
    ```

4.  **ExecuÃ§Ã£o das TransformaÃ§Ãµes (SQL):**
    * Rodar `sql/etl_clean_funnel.sql` no console do BigQuery.
    * Rodar `sql/data_contract_check.sql` para validar integridade.
    * Rodar `sql/etl_delivery_dashboard.sql` para gerar a tabela final.

### ğŸ”„ OrquestraÃ§Ã£o (Opcional)
Foi desenhada uma DAG do **Airflow** (`dags/marketing_funnel_dag.py`) que automatiza o fluxo:
`IngestÃ£o -> Limpeza (Silver) -> Data Contract Check -> Entrega (Gold)`

## âœ… Qualidade de Dados (Data Contracts)
Foi implementada uma etapa de validaÃ§Ã£o (`sql/data_contract_check.sql`) que atua como *Circuit Breaker*. O pipeline Ã© interrompido caso:
* Existam IDs nulos.
* **InconsistÃªncia Temporal:** Data de conversÃ£o MQL anterior Ã  criaÃ§Ã£o do Lead.
* Valores de ICP Score fora do domÃ­nio (A-D).

## ğŸ“Š EntregÃ¡veis de NegÃ³cio
As queries para responder Ã s perguntas estratÃ©gicas (Top Campanhas, Leads HR) encontram-se no arquivo:
ğŸ“‚ `sql/business_questions.sql`

O CSV final com os dados processados para o Dashboard estÃ¡ disponÃ­vel na raiz como:
ğŸ“„ `entregavel_metricas_finais.csv`

---
Desenvolvido por **Bianca Rodrigues Soares Costa**